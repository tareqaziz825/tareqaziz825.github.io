<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>M. S. B. Siddiqui - Publications</title>
    <!-- CSS Links etc. -->
    <link href="https://cdn.jsdelivr.net/npm/bootstrap@5.1.3/dist/css/bootstrap.min.css" rel="stylesheet">
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.0.0/css/all.min.css">
    <link href="https://fonts.googleapis.com/css2?family=Roboto:wght@300;400;500;700&display=swap" rel="stylesheet">
    <link rel="stylesheet" href="styles.css">
</head>
<body>
    <nav class="navbar navbar-expand-lg navbar-light fixed-top">
        <div class="container">
            <a class="navbar-brand" href="index.html">M. S. B. Siddiqui</a>
            <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation">
                <span class="navbar-toggler-icon"></span>
            </button>
            <div class="collapse navbar-collapse" id="navbarNav">
                <ul class="navbar-nav ms-auto">
                    <li class="nav-item"><a class="nav-link" href="index.html">About</a></li>
                    <li class="nav-item"><a class="nav-link" href="research.html">Research</a></li>
                    <li class="nav-item"><a class="nav-link active" href="publications.html">Publications</a></li>
                    <li class="nav-item"><a class="nav-link" href="teaching.html">Teaching</a></li>
                    <li class="nav-item"><a class="nav-link" href="experience.html">Experience</a></li>
                    <li class="nav-item"><a class="nav-link" href="qualifications.html">Credentials</a></li>
                    <li class="nav-item"><a class="nav-link" href="awards.html">Awards</a></li>
                    <li class="nav-item"><a class="nav-link" href="others.html">Others</a></li>
                </ul>
                <button id="theme-toggle" class="btn theme-toggle-btn ms-2">
                    <i class="fas fa-moon"></i>
                </button>
            </div>
        </div>
    </nav>

    <!-- Main Content -->
    <main>
        <section id="publications" class="section fade-in">
            <div class="container">
                <h2 class="mb-5">Publications & Preprints</h2>

                <!-- Google Scholar Introductory Paragraph and Button -->
                <div class="text-center mb-4">
                    <p class="lead">Below is a curated list of my publications. For a comprehensive and continuously updated list, including citation metrics, please refer to my official Google Scholar profile.</p>
                </div>
                <div class="text-center mb-5">
                    <a href="https://scholar.google.com/citations?user=kSXa-48AAAAJ&hl=en" class="btn btn-outline-primary" target="_blank" rel="noopener noreferrer">
                        <i class="fas fa-graduation-cap me-2"></i> Google Scholar
                    </a>
                </div>
                
                <!-- Journal Articles Section -->
                <h3 class="mb-4">Journal Articles</h3>
                <div class="publications-list mb-5">
                    <!-- Pub 1: DUA-D2C -->
                    <div class="publication-item">
                        <div class="publication-meta">
                            <span class="pub-type"><i class="fas fa-book-open"></i> Journal Article</span>
                            <span class="pub-year"><i class="fas fa-calendar-alt"></i> 2025</span>
                            <span class="pub-status"><i class="fas fa-circle text-warning me-2"></i>Under Review</span>
                        </div>
                        <div class="row align-items-center">
                            <div class="col-md-9">
                                <h4>DUA-D2C: Dynamic Uncertainty Aware Overfitting Remediation in Deep Learning</h4>
                                <p class="authors"><i class="fas fa-users"></i> <strong>M. S. B. Siddiqui</strong>, M. M. Islam, M. G. R. Alam</p>
                                <p class="venue"><i class="fas fa-university"></i> Under review at&nbsp;<em>Complex & Intelligent Systems</em> </p>
                            </div>
                            <div class="col-md-3 text-md-end">
                                <div class="publication-links">
                                    <button class="btn btn-outline-primary btn-sm" data-bs-toggle="modal" data-bs-target="#abstractModal1"><i class="fas fa-align-left"></i> Abstract</button>
                                    <a href="https://arxiv.org/abs/2411.15876" class="btn btn-outline-primary btn-sm" target="_blank"><i class="fas fa-external-link-alt"></i> Paper</a>
                                    <a href="https://github.com/Saiful185/DUA-D2C" class="btn btn-outline-primary btn-sm" target="_blank"><i class="fab fa-github"></i> Code</a>
                                </div>
                            </div>
                        </div>
                    </div>
                    <!-- Pub 2: S3F-Net -->
                    <div class="publication-item">
                        <div class="publication-meta">
                            <span class="pub-type"><i class="fas fa-book-open"></i> Journal Article</span>
                            <span class="pub-year"><i class="fas fa-calendar-alt"></i> 2025</span>
                            <span class="pub-status"><i class="fas fa-circle text-warning me-2"></i>Under Review</span>
                        </div>
                        <div class="row align-items-center">
                            <div class="col-md-9">
                                <h4>S³F-Net: A Multi-Modal Approach to Medical Image Classification via Spatial-Spectral Summarizer Fusion Network</h4>
                                <p class="authors"><i class="fas fa-users"></i> <strong>M. S. B. Siddiqui</strong>, M. I. H. Bhuiyan</p>
                                <p class="venue"><i class="fas fa-university"></i> Under review at&nbsp;<em>IEEE Journal of Biomedical and Health Informatics</em></p>
                            </div>
                            <div class="col-md-3 text-md-end">
                                <div class="publication-links">
                                    <button class="btn btn-outline-primary btn-sm" data-bs-toggle="modal" data-bs-target="#abstractModal2"><i class="fas fa-align-left"></i> Abstract</button>
                                    <a href="https://arxiv.org/abs/2509.23442" class="btn btn-outline-primary btn-sm" target="_blank"><i class="fas fa-external-link-alt"></i> Paper</a>
                                    <a href="https://github.com/Saiful185/S3F-Net" class="btn btn-outline-primary btn-sm" target="_blank"><i class="fab fa-github"></i> Code</a>
                                </div>
                            </div>
                        </div>
                    </div>
                     <!-- Pub 3: Mixed-Methods -->
                    <div class="publication-item">
                        <div class="publication-meta">
                            <span class="pub-type"><i class="fas fa-book-open"></i> Journal Article</span>
                            <span class="pub-year"><i class="fas fa-calendar-alt"></i> 2025</span>
                            <span class="pub-status"><i class="fas fa-circle text-warning me-2"></i>Under Review</span>
                        </div>
                        <div class="row align-items-center">
                            <div class="col-md-9">
                                <h4>A Mixed-Methods Analysis of Repression and Mobilization in Bangladesh's July Revolution Using Machine Learning and Statistical Modeling</h4>
                                <p class="authors"><i class="fas fa-users"></i> <strong>M. S. B. Siddiqui</strong>, A. D. Roy</p>
                                <p class="venue"><i class="fas fa-university"></i> Under review at&nbsp;<em> Social Forces (Presented at IPB Annual Conference 2025, Bremen, Germany)</em></p>
                            </div>
                            <div class="col-md-3 text-md-end">
                                <div class="publication-links">
                                    <button class="btn btn-outline-primary btn-sm" data-bs-toggle="modal" data-bs-target="#abstractModal3"><i class="fas fa-align-left"></i> Abstract</button>
                                    <a href="https://arxiv.org/abs/2510.06264" class="btn btn-outline-primary btn-sm" target="_blank"><i class="fas fa-external-link-alt"></i> Paper</a>
                                    <a href="https://github.com/Saiful185/July-Revolution-Analysis" class="btn btn-outline-primary btn-sm" target="_blank"><i class="fab fa-github"></i> Code</a>
                                </div>
                            </div>
                        </div>
                    </div>
                </div>

                <!-- Conference Papers Section -->
                <h3 class="mb-4">Conference Papers</h3>
                <div class="publications-list">
                    <!-- Pub 4: AMR-EnsembleNet -->
                    <div class="publication-item">
                        <div class="publication-meta">
                            <span class="pub-type"><i class="fas fa-book"></i> Conference Paper</span>
                            <span class="pub-year"><i class="fas fa-calendar-alt"></i> 2026</span>
                            <span class="pub-status"><i class="fas fa-circle text-info me-2"></i>Accepted</span>
                        </div>
                        <div class="row align-items-center">
                            <div class="col-md-9">
                                <h4>Fusing Sequence Motifs and Pan-Genomic Features: Antimicrobial Resistance Prediction using an Explainable Lightweight 1D CNN - XGBoost Ensemble</h4>
                                <p class="authors"><i class="fas fa-users"></i> <strong>M. S. B. Siddiqui</strong>, N. Tarannum</p>
                                <p class="venue"><i class="fas fa-university"></i> To appear in&nbsp;<em>2026 SupercomputingAsia / High Performance Computing in Asia-Pacific Region (SCA/HPCAsia)</em></p>
                            </div>
                            <div class="col-md-3 text-md-end">
                                <div class="publication-links">
                                    <button class="btn btn-outline-primary btn-sm" data-bs-toggle="modal" data-bs-target="#abstractModal4"><i class="fas fa-align-left"></i> Abstract</button>
                                    <a href="https://www.biorxiv.org/content/early/2025/09/27/2025.09.27.678993" class="btn btn-outline-primary btn-sm" target="_blank"><i class="fas fa-external-link-alt"></i> Paper</a>
                                    <a href="https://github.com/Saiful185/AMR-EnsembleNet" class="btn btn-outline-primary btn-sm" target="_blank"><i class="fab fa-github"></i> Code</a>
                                </div>
                            </div>
                        </div>
                    </div>
                     <!-- Pub 5: AudioFuse -->
                    <div class="publication-item">
                        <div class="publication-meta">
                            <span class="pub-type"><i class="fas fa-book"></i> Conference Paper</span>
                            <span class="pub-year"><i class="fas fa-calendar-alt"></i> 2025</span>
                            <span class="pub-status"><i class="fas fa-circle text-warning me-2"></i>Under Review</span>
                        </div>
                        <div class="row align-items-center">
                            <div class="col-md-9">
                                <h4>AudioFuse: Unified Spectral-Temporal Learning via a Hybrid ViT-1D CNN Architecture for Robust Phonocardiogram Classification</h4>
                                <p class="authors"><i class="fas fa-users"></i> <strong>M. S. B. Siddiqui</strong>, U. Saha</p>
                                <p class="venue"><i class="fas fa-university"></i> Under review at&nbsp;<em>2026 IEEE International Conference on Acoustics, Speech, and Signal Processing (ICASSP)</em></p>
                            </div>
                            <div class="col-md-3 text-md-end">
                                <div class="publication-links">
                                    <button class="btn btn-outline-primary btn-sm" data-bs-toggle="modal" data-bs-target="#abstractModal5"><i class="fas fa-align-left"></i> Abstract</button>
                                    <a href="https://arxiv.org/abs/2509.23454" class="btn btn-outline-primary btn-sm" target="_blank"><i class="fas fa-external-link-alt"></i> Paper</a>
                                    <a href="https://github.com/Saiful185/AudioFuse" class="btn btn-outline-primary btn-sm" target="_blank"><i class="fab fa-github"></i> Code</a>
                                </div>
                            </div>
                        </div>
                    </div>
                     <!-- Pub 6: D2C -->
                    <div class="publication-item">
                        <div class="publication-meta">
                            <span class="pub-type"><i class="fas fa-book"></i> Conference Paper</span>
                            <span class="pub-year"><i class="fas fa-calendar-alt"></i> 2024</span>
                            <span class="pub-status"><i class="fas fa-circle text-success me-2"></i>Published</span>
                        </div>
                        <div class="row align-items-center">
                            <div class="col-md-9">
                                <h4>Divide2Conquer (D2C): A Decentralized Approach Towards Overfitting Remediation in Deep Learning</h4>
                                <p class="authors"><i class="fas fa-users"></i> <strong>M. S. B. Siddiqui</strong>, M. M. Islam, M. G. R. Alam</p>
                                <p class="venue"><i class="fas fa-university"></i> 2024 IEEE International Conference on Big Data (BigData)</p>
                            </div>
                            <div class="col-md-3 text-md-end">
                                <div class="publication-links">
                                    <button class="btn btn-outline-primary btn-sm" data-bs-toggle="modal" data-bs-target="#abstractModal6"><i class="fas fa-align-left"></i> Abstract</button>
                                    <a href="https://doi.org/10.1109/BigData62323.2024.10826082" class="btn btn-outline-primary btn-sm" target="_blank"><i class="fas fa-external-link-alt"></i> Paper</a>
                                    <a href="https://github.com/Saiful185/Divide2Conquer" class="btn btn-outline-primary btn-sm" target="_blank"><i class="fab fa-github"></i> Code</a>
                                </div>
                            </div>
                        </div>
                    </div>
                    <!-- Pub 7: FedNet -->
                    <div class="publication-item">
                        <div class="publication-meta">
                            <span class="pub-type"><i class="fas fa-book"></i> Conference Paper</span>
                            <span class="pub-year"><i class="fas fa-calendar-alt"></i> 2022</span>
                            <span class="pub-status"><i class="fas fa-circle text-success me-2"></i>Published</span>
                        </div>
                        <div class="row align-items-center">
                            <div class="col-md-9">
                                <h4>FedNet: Federated Implementation of Neural Networks for Facial Expression Recognition</h4>
                                <p class="authors"><i class="fas fa-users"></i> <strong>M. S. B. Siddiqui</strong>, S. A. Shusmita, S. Sabreen, M. G. R. Alam</p>
                                <p class="venue"><i class="fas fa-university"></i> 2022 International Conference on Decision Aid Sciences and Applications (DASA)</p>
                            </div>
                            <div class="col-md-3 text-md-end">
                                <div class="publication-links">
                                    <button class="btn btn-outline-primary btn-sm" data-bs-toggle="modal" data-bs-target="#abstractModal7"><i class="fas fa-align-left"></i> Abstract</button>
                                    <a href="https://doi.org/10.1109/DASA54658.2022.9765165" class="btn btn-outline-primary btn-sm" target="_blank"><i class="fas fa-external-link-alt"></i> Paper</a>
                                    <a href="https://github.com/Saiful185/FedNet-Federated-Implementation-of-CNN-for-Facial-Expression-Recognition" class="btn btn-outline-primary btn-sm" target="_blank"><i class="fab fa-github"></i> Code</a>
                                </div>
                            </div>
                        </div>
                    </div>
                     <!-- Pub 8: Bioradiolocation -->
                    <div class="publication-item">
                        <div class="publication-meta">
                            <span class="pub-type"><i class="fas fa-book"></i> Conference Paper</span>
                            <span class="pub-year"><i class="fas fa-calendar-alt"></i> 2022</span>
                            <span class="pub-status"><i class="fas fa-circle text-success me-2"></i>Published</span>
                        </div>
                        <div class="row align-items-center">
                            <div class="col-md-9">
                                <h4>Bioradiolocation-Based Multi-Class Sleep Stage Classification Using Time and Frequency Features with Random Forest Classifier</h4>
                                <p class="authors"><i class="fas fa-users"></i> M. S. I. Siam,&nbsp;<strong> M. S. B. Siddiqui</strong>, M. Abedin, M. I. H. Bhuiyan</p>
                                <p class="venue"><i class="fas fa-university"></i> 2022 12th International Conference on Electrical and Computer Engineering (ICECE)</p>
                            </div>
                            <div class="col-md-3 text-md-end">
                                <div class="publication-links">
                                    <button class="btn btn-outline-primary btn-sm" data-bs-toggle="modal" data-bs-target="#abstractModal8"><i class="fas fa-align-left"></i> Abstract</button>
                                    <a href="https://doi.org/10.1109/ICECE57408.2022.10089093" class="btn btn-outline-primary btn-sm" target="_blank"><i class="fas fa-external-link-alt"></i> Paper</a>
                                </div>
                            </div>
                        </div>
                    </div>
                </div>

            </div>
        </section>

        <!-- Connect Section -->
        <section id="connect" class="section fade-in">
             <div class="container">
                <h2>Connect</h2>
                <div class="row">
                    <div class="col-12">
                        <div class="insight-card connect text-center">
                            <h4><i class="fas fa-globe"></i> Get in Touch</h4>
                            <p>Find me on academic and professional networks, or reach out via email.</p>
                            <div class="social-links">
                                <a href="https://github.com/Saiful185" class="btn social-btn github" title="GitHub" target="_blank" rel="noopener noreferrer"><i class="fab fa-github"></i> GitHub</a>
                                <a href="https://www.linkedin.com/in/md-saiful-bari-siddiqui-8aba351a8/" class="btn social-btn linkedin" title="LinkedIn" target="_blank" rel="noopener noreferrer"><i class="fab fa-linkedin"></i> LinkedIn</a>
                                <a href="https://scholar.google.com/citations?user=kSXa-48AAAAJ&hl=en" class="btn social-btn scholar" title="Google Scholar" target="_blank" rel="noopener noreferrer"><i class="fas fa-graduation-cap"></i> Google Scholar</a>
                                <a href="https://www.youtube.com/@saifulbariiftu/playlists" class="btn social-btn youtube" title="YouTube" target="_blank" rel="noopener noreferrer"><i class="fab fa-youtube"></i> YouTube</a>
                            </div>
                            <div class="contact-info mt-4">
                                <p><i class="fas fa-envelope"></i> <a href="mailto:saiful.bari@bracu.ac.bd">saiful.bari@bracu.ac.bd</a></p>
                                <p><i class="fas fa-map-marker-alt"></i>North Badda, Dhaka-1212, Bangladesh</p>
                                <p><i class="fas fa-phone"></i> +880-1758805835</p>
                            </div>
                        </div>
                    </div>
                </div>
            </div>
        </section>
    </main>

    <!-- Abstract Modals -->
    <!-- Modal 1 -->
    <div class="modal fade" id="abstractModal1" tabindex="-1"><div class="modal-dialog modal-lg modal-dialog-centered"><div class="modal-content"><div class="modal-header"><h5 class="modal-title">Abstract</h5><button type="button" class="btn-close" data-bs-dismiss="modal"></button></div><div class="modal-body"><p>Overfitting remains a significant challenge in deep learning, often arising from data outliers, noise, and limited training data. To address this, the Divide2Conquer (D2C) method was previously proposed, which partitions training data into multiple subsets and trains identical models independently on each. This strategy enables learning more consistent patterns while minimizing the influence of individual outliers and noise. However, D2C's standard aggregation typically treats all subset models equally or based on fixed heuristics (like data size), potentially underutilizing information about their varying generalization capabilities. Building upon this foundation, we introduce Dynamic Uncertainty-Aware Divide2Conquer (DUA-D2C), an advanced technique that refines the aggregation process. DUA-D2C dynamically weights the contributions of subset models based on their performance on a shared validation set, considering both accuracy and prediction uncertainty. This intelligent aggregation allows the central model to preferentially learn from subsets yielding more generalizable and confident edge models, thereby more effectively combating overfitting. Empirical evaluations on benchmark datasets spanning multiple domains demonstrate that DUA-D2C significantly improves generalization. Our analysis includes evaluations of decision boundaries, loss curves, and other performance metrics, highlighting the effectiveness of DUA-D2C. This study demonstrates that DUA-D2C improves generalization performance even when applied on top of other regularization methods, establishing it as a theoretically grounded and effective approach to combating overfitting in modern deep learning.</p></div></div></div></div>
    <!-- Modal 2 -->
    <div class="modal fade" id="abstractModal2" tabindex="-1"><div class="modal-dialog modal-lg modal-dialog-centered"><div class="modal-content"><div class="modal-header"><h5 class="modal-title">Abstract</h5><button type="button" class="btn-close" data-bs-dismiss="modal"></button></div><div class="modal-body"><p>Convolutional Neural Networks (CNNs) have become a cornerstone of medical image analysis due to their proficiency in learning hierarchical spatial features. However, this focus on a single domain is inefficient at capturing global, holistic patterns and fails to explicitly model an image's frequency-domain characteristics. To address these challenges, we propose the Spatial-Spectral Summarizer Fusion Network (S³F-Net), a dual-branch framework that learns from both spatial and spectral representations simultaneously. The S³F-Net performs a fusion of a deep spatial CNN with our proposed shallow spectral encoder, SpectraNet. SpectraNet features the proposed SpectralFilter layer, which leverages the Convolution Theorem by applying a bank of learnable filters directly to an image's full Fourier spectrum via a computation-efficient element-wise multiplication. This allows the SpectralFilter layer to attain a global receptive field instantaneously, with its output being distilled by a lightweight summarizer network. We evaluate S³F-Net across four diverse medical imaging datasets spanning different scales and modalities: HAM10000 (dermoscopy), BUSI (ultrasound), BRISC2025 (MRI), and Chest X-Ray Pneumonia (radiography), to validate its efficacy and generalizability, and reveal the task-dependent nature of the optimal fusion strategy. Our framework consistently and significantly outperforms its strong spatial-only baseline in all cases, with accuracy improvements of up to 5.13%. With a powerful Bilinear Fusion, S³F-Net achieves a state-of-the-art competitive accuracy of 98.76% on the BRISC2025 dataset. A simpler Concatenation Fusion performs better on the texture-dominant Chest X-Ray Pneumonia dataset, achieving 93.11% accuracy, surpassing many top-performing, much deeper models. Our explainability analysis also reveals that the S³F-Net learns to dynamically adjust its reliance on each branch based on the input pathology. These results verify that our dual-domain approach is a powerful and generalizable paradigm for medical image analysis.</p></div></div></div></div>
    <!-- Modal 3 -->
    <div class="modal fade" id="abstractModal3" tabindex="-1"><div class="modal-dialog modal-lg modal-dialog-centered"><div class="modal-content"><div class="modal-header"><h5 class="modal-title">Abstract</h5><button type="button" class="btn-close" data-bs-dismiss="modal"></button></div><div class="modal-body"><p>The 2024 July Revolution in Bangladesh represents a landmark event in the study of civil resistance: a successful, student-led civilian uprising that overthrew a long-standing authoritarian regime. This study investigates the central paradox of its success: how state repression, intended to quell dissent, ultimately fueled the movement's victory. First, we develop a qualitative narrative of the conflict's timeline to generate specific, testable hypotheses. Then, using a disaggregated, event-level dataset, we employ a multi-method quantitative analysis to dissect the complex relationship between repression and mobilisation, providing a reproducible framework to analyse explosive modern uprisings like the July Revolution. Initial pooled regression highlights the crucial role of protest momentum (feedback loop effect) in sustaining the movement. To isolate causal effects, we specify a Two-Way Fixed Effects panel model, which provides robust evidence for a direct and statistically significant local suppression backfire effect. Our Vector Autoregression (VAR) analysis provides clear visual evidence of an immediate, nationwide mobilisation in response to increased lethal violence. A structural break analysis reveals that the backfire dynamic was non-linear, statistically insignificant in the conflict's early phase, but triggered by the catalytic moral shock of the first wave of lethal violence, and its visuals circulated around July 16th. A complementary machine learning analysis corroborates this finding from a predictive standpoint. We conclude that the July Revolution was driven by a contingent, non-linear backfire, triggered by specific catalytic moral shocks and accelerated by the viral reaction to the visual spectacle of state brutality.</p></div></div></div></div>
    <!-- Modal 4 -->
    <div class="modal fade" id="abstractModal4" tabindex="-1"><div class="modal-dialog modal-lg modal-dialog-centered"><div class="modal-content"><div class="modal-header"><h5 class="modal-title">Abstract</h5><button type="button" class="btn-close" data-bs-dismiss="modal"></button></div><div class="modal-body"><p>Antimicrobial Resistance (AMR) is a rapidly escalating global health crisis. While genomic sequencing enables rapid prediction of resistance phenotypes, current computational methods have limitations. Standard machine learning models treat the genome as an unordered collection of features, ignoring the sequential context of Single Nucleotide Polymorphisms (SNPs). State-of-the-art sequence models like Transformers are often too data-hungry and computationally expensive for the moderately-sized datasets that are typical in this domain. To address these challenges, we propose AMR-EnsembleNet, an ensemble framework that synergistically combines sequence-based and feature-based learning. We developed a lightweight, custom 1D Convolutional Neural Network (CNN) to efficiently learn predictive sequence motifs from high-dimensional SNP data. This sequence-aware model was ensembled with an XGBoost model, a powerful gradient boosting system adept at capturing complex, non-local feature interactions. We trained and evaluated our framework on a benchmark dataset of 809 E. coli strains, predicting resistance across four antibiotics with varying class imbalance. Our 1D CNN-XGBoost ensemble consistently achieved top-tier performance across all the antibiotics, reaching a Matthews Correlation Coefficient (MCC) of 0.926 for Ciprofloxacin (CIP) and the highest Macro F1-score of 0.691 for the challenging Gentamicin (GEN) AMR prediction. We also show that our model consistently focuses on SNPs within well-known AMR genes like fusA and parC, confirming it learns the correct genetic signals for resistance. Our work demonstrates that fusing a sequence-aware 1D CNN with a feature-based XGBoost model creates a powerful ensemble, overcoming the limitations of using either an order-agnostic or a standalone sequence model.</p></div></div></div></div>
    <!-- Modal 5 -->
    <div class="modal fade" id="abstractModal5" tabindex="-1"><div class="modal-dialog modal-lg modal-dialog-centered"><div class="modal-content"><div class="modal-header"><h5 class="modal-title">Abstract</h5><button type="button" class="btn-close" data-bs-dismiss="modal"></button></div><div class="modal-body"><p>Biomedical audio signals, such as phonocardiograms (PCG), are inherently rhythmic and contain diagnostic information in both their spectral (tonal) and temporal domains. Standard 2D spectrograms provide rich spectral features but compromise the phase information and temporal precision of the 1D waveform. We propose AudioFuse, an architecture that simultaneously learns from both complementary representations to classify PCGs. To mitigate the overfitting risk common in fusion models, we integrate a custom, wide-and-shallow Vision Transformer (ViT) for spectrograms with a shallow 1D CNN for raw waveforms. On the PhysioNet 2016 dataset, AudioFuse achieves a state-of-the-art competitive ROC-AUC of 0.8608 when trained from scratch, outperforming its spectrogram (0.8066) and waveform (0.8223) baselines. Moreover, it demonstrates superior robustness to domain shift on the challenging PASCAL dataset, maintaining an ROC-AUC of 0.7181 while the spectrogram baseline collapses (0.4873). Fusing complementary representations thus provides a strong inductive bias, enabling the creation of efficient, generalizable classifiers without requiring large-scale pre-training.</p></div></div></div></div>
    <!-- Modal 6 -->
    <div class="modal fade" id="abstractModal6" tabindex="-1"><div class="modal-dialog modal-lg modal-dialog-centered"><div class="modal-content"><div class="modal-header"><h5 class="modal-title">Abstract</h5><button type="button" class="btn-close" data-bs-dismiss="modal"></button></div><div class="modal-body"><p>Overfitting remains a persistent challenge in deep learning. It is primarily attributed to data outliers, noise, and limited training set sizes. This paper presents Divide2Conquer (D2C), a novel technique designed to address this issue. D2C proposes partitioning the training data into multiple subsets and training separate identical models on them. To avoid overfitting on any specific subset, the trained parameters from these models are aggregated and averaged periodically throughout the training phase, enabling the model to learn from the entire dataset while mitigating the impact of individual outliers or noise. Empirical evaluations on multiple benchmark datasets across various deep learning tasks demonstrate that D2C effectively improves generalization performance, particularly for larger datasets. This study verifies D2C’s ability to achieve significant performance gains both as a standalone technique and when used in conjunction with other overfitting reduction methods through a series of experiments, including analysis of decision boundaries, loss curves, and other performance metrics. It also provides valuable insights into the implementation and hyperparameter tuning of D2C.</p></div></div></div></div>
    <!-- Modal 7 -->
    <div class="modal fade" id="abstractModal7" tabindex="-1"><div class="modal-dialog modal-lg modal-dialog-centered"><div class="modal-content"><div class="modal-header"><h5 class="modal-title">Abstract</h5><button type="button" class="btn-close" data-bs-dismiss="modal"></button></div><div class="modal-body"><p>Overfitting is a significant obstacle in classification tasks like Facial Expression Recognition. Despite repeated attempts to negate the effects of overfitting by various researchers, the issue persists. This paper introduces FedNet, a Neural Network architecture inspired by the federated aggregation and averaging technique used in Federated optimization, which proposes a method to learn from data situated separately in edge devices. Our study proposes dividing training data into multiple shards and training each data shard individually using the same neural network, then aggregating and averaging each model’s parameters after every few epochs. Training multiple models on different data shards and averaging the learned parameters allows the model to learn from the entire dataset while avoiding overfitting on a particular data shard. Convolutional Neural Networks were implemented using the Extended Cohn Kanade and the FER-2013 dataset to conduct this study. Our federated averaging-based implementation of CNNs achieved 99.1% accuracy and 100% accuracy for 8 and 7 emotion classifications, respectively, on CK+, beating the benchmarks for this dataset. It also achieved 65.6% accuracy on FER-2013 without using any transfer learning or data augmentation. Our model shows significantly better resistance against overfitting, resulting in better generalization compared to the other existing methods.</p></div></div></div></div>
    <!-- Modal 8 -->
    <div class="modal fade" id="abstractModal8" tabindex="-1"><div class="modal-dialog modal-lg modal-dialog-centered"><div class="modal-content"><div class="modal-header"><h5 class="modal-title">Abstract</h5><button type="button" class="btn-close" data-bs-dismiss="modal"></button></div><div class="modal-body"><p>Sleep disorders are a common problem that disrupts our regular sleeping patterns. To diagnose sleep disorders, Long-term monitoring of sleep could be useful. In this paper an automated scheme of sleep staging is presented based on Bioradiolocation signals using time and frequency domain feature extraction and Random Forest Classifier. This experiment is validated using data of 32 subjects without sleep-related breathing disorders. A Random Forest based algorithm is used for two, three, four and five-stage classification. We achieved the best performance so far (89.35% accuracy and 0.65 Cohens kappa) on 2-stage, 75.3% accuracy on 3-stage, 56.18% on 4-stage, and 54.2% accuracy on 5-stage classification with BRL Signals. These results show high potential in real-life sleep stage monitoring systems.</p></div></div></div></div>

    <!-- Footer -->
    <footer class="text-center">
        <div class="container">
            <p>&copy; 2025 Md. Saiful Bari Siddiqui. All rights reserved.</p>
        </div>
    </footer>

    <!-- JS Scripts (Same as index.html) -->
    <script src="https://cdn.jsdelivr.net/npm/bootstrap@5.1.3/dist/js/bootstrap.bundle.min.js"></script>
    <script>
        const fadeElements = document.querySelectorAll('.fade-in');
        const observer = new IntersectionObserver(entries => {
            entries.forEach(entry => {
                if (entry.isIntersecting) {
                    entry.target.classList.add('visible');
                }
            });
        });
        fadeElements.forEach(element => {
            observer.observe(element);
        });
    </script>
<!-- Dark Mode Toggle & Persistence Script -->
<script>
    const themeToggle = document.getElementById('theme-toggle');
    const body = document.body;
    const icon = themeToggle.querySelector('i');

    // Function to apply the saved theme on page load
    const applyTheme = () => {
        const savedTheme = localStorage.getItem('theme');
        if (savedTheme === 'dark') {
            body.classList.add('dark-mode');
            icon.className = 'fas fa-sun';
        } else {
            body.classList.remove('dark-mode');
            icon.className = 'fas fa-moon';
        }
    };

    // Event listener for the toggle button
    themeToggle.addEventListener('click', () => {
        body.classList.toggle('dark-mode');
        // Save the theme preference to localStorage
        if (body.classList.contains('dark-mode')) {
            localStorage.setItem('theme', 'dark');
            icon.className = 'fas fa-sun';
        } else {
            localStorage.setItem('theme', 'light');
            icon.className = 'fas fa-moon';
        }
    });

    // Apply the theme when the DOM is fully loaded
    document.addEventListener('DOMContentLoaded', applyTheme);
</script>
</body>
</html>
